{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd22de-a092-49ec-a65f-efb966eb2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import hstack\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, label_binarize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, classification_report,\n",
    "    confusion_matrix, roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1579acff-7097-4286-8073-16d5623dc5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL data:  Unnamed: 0            0\n",
      "Danceability          2\n",
      "Energy                2\n",
      "Key                   2\n",
      "Loudness              2\n",
      "Speechiness           2\n",
      "Acousticness          2\n",
      "Instrumentalness      2\n",
      "Liveness              2\n",
      "Valence               2\n",
      "Tempo                 2\n",
      "Duration_ms           2\n",
      "Views               470\n",
      "Likes               541\n",
      "Comments            569\n",
      "Licensed            470\n",
      "official_video      470\n",
      "Stream              576\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritvi\\AppData\\Local\\Temp\\ipykernel_21472\\2456761259.py:31: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(df[col].mode()[0])\n"
     ]
    }
   ],
   "source": [
    "#read dataset\n",
    "df = pd.read_csv(\"Spotify_Youtube.csv\")\n",
    "\n",
    "#display first examples\n",
    "df.head()\n",
    "\n",
    "#Dropping features from the dataset that add no predictive value to the model\n",
    "df = df.drop(columns=[\n",
    "    'Track', 'Artist', 'Url_spotify', 'Uri', 'Url_youtube', 'Title', 'Channel', 'Description', 'Album', 'Album_type'])\n",
    "\n",
    "#Sanity check and dropping duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#Check for missing data\n",
    "print(\"NULL data: \",df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing target features\n",
    "df = df.dropna(subset=['Views'])\n",
    "\n",
    "# Fill numeric columns with median\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_cols_wo_views = [c for c in numeric_cols if c != 'Views']\n",
    "df[numeric_cols_wo_views] = df[numeric_cols_wo_views].fillna(df[numeric_cols_wo_views].median())\n",
    "\n",
    "# Fill categorical/boolean columns with mode\n",
    "categorical_cols = df.select_dtypes(include=['object', 'bool']).columns\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Create popularity classes based on quantiles\n",
    "df['popularity_class'] = pd.qcut(df['Views'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Drop the original 'views' column (to avoid leakage of target)\n",
    "df = df.drop(columns=['Views'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b361121-ad14-4cb0-9e7c-2ac935f28ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train shape: (16198, 17)\n",
      "Final test shape: (4050, 17)\n",
      "********BASELINE Logistic Regression (no tuning)********\n",
      "Accuracy: 0.8432\n",
      "Precision: 0.8466\n",
      "Recall: 0.8432\n",
      "F1: 0.8425\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        High       0.94      0.83      0.88      1350\n",
      "         Low       0.83      0.95      0.89      1350\n",
      "      Medium       0.77      0.75      0.76      1350\n",
      "\n",
      "    accuracy                           0.84      4050\n",
      "   macro avg       0.85      0.84      0.84      4050\n",
      "weighted avg       0.85      0.84      0.84      4050\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[1122    1  227]\n",
      " [   0 1281   69]\n",
      " [  76  262 1012]]\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best params: {'C': 10, 'class_weight': None, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Best CV f1_macro: 0.8516163727237446\n",
      "********TUNED (GridSearchCV)********\n",
      "Accuracy: 0.8573\n",
      "Precision (macro): 0.8598\n",
      "Recall (macro): 0.8573\n",
      "F1 (macro): 0.8572\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        High       0.93      0.84      0.88      1350\n",
      "         Low       0.86      0.94      0.90      1350\n",
      "      Medium       0.78      0.79      0.79      1350\n",
      "\n",
      "    accuracy                           0.86      4050\n",
      "   macro avg       0.86      0.86      0.86      4050\n",
      "weighted avg       0.86      0.86      0.86      4050\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[1130    1  219]\n",
      " [   0 1273   77]\n",
      " [  83  198 1069]]\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "#Feature and Target split \n",
    "target_col = 'popularity_class'\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "#identifying numerical features, bool = numerical as Pandas stores True=1 and False=0\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64','bool']).columns.tolist()\n",
    "#Identifying any categorical features\n",
    "categorical_features = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "# 2) Train/test split (stratified to preserve Low/Medium/High ratios)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "#Scaling numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test_num = scaler.transform(X_test[numeric_features])\n",
    "X_train_cat = X_train[categorical_features]\n",
    "X_test_cat = X_test[categorical_features]\n",
    "\n",
    "#Recombining numerical + categorical data\n",
    "X_train_final = hstack([X_train_num, X_train_cat])\n",
    "X_test_final = hstack([X_test_num, X_test_cat])\n",
    "\n",
    "print(\"Final train shape:\", X_train_final.shape)\n",
    "print(\"Final test shape:\", X_test_final.shape)\n",
    "\n",
    "#Performing Baseline Logistic Regression\n",
    "baseline_lr = LogisticRegression(max_iter=1000,solver='lbfgs',random_state=RANDOM_STATE)\n",
    "baseline_lr.fit(X_train_final, y_train)\n",
    "y_pred_base = baseline_lr.predict(X_test_final)\n",
    "\n",
    "acc_b  = accuracy_score(y_test, y_pred_base)\n",
    "prec_b, rec_b, f1_b, _ = precision_recall_fscore_support(y_test, y_pred_base, average='macro', zero_division=0)\n",
    "\n",
    "print(\"********BASELINE Logistic Regression (no tuning)********\")\n",
    "print(f\"Accuracy: {acc_b:.4f}\")\n",
    "print(f\"Precision: {prec_b:.4f}\")\n",
    "print(f\"Recall: {rec_b:.4f}\")\n",
    "print(f\"F1: {f1_b:.4f}\\n\")\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred_base, zero_division=0))\n",
    "\n",
    "cm_b = confusion_matrix(y_test, y_pred_base, labels=sorted(y.unique()))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm_b)\n",
    "\n",
    "#Performing logistic regression with GridSearchCV\n",
    "log_reg = LogisticRegression(max_iter=1000,solver='lbfgs',random_state=RANDOM_STATE)\n",
    "\n",
    "#tuning parameters\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'newton-cg', 'saga'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=baseline_lr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV f1_macro:\", grid.best_score_)\n",
    "\n",
    "best_lr = grid.best_estimator_\n",
    "y_pred_tuned = best_lr.predict(X_test_final)\n",
    "\n",
    "acc_t  = accuracy_score(y_test, y_pred_tuned)\n",
    "prec_t, rec_t, f1_t, _ = precision_recall_fscore_support(y_test, y_pred_tuned, average='macro', zero_division=0)\n",
    "\n",
    "print(\"********TUNED (GridSearchCV)********\")\n",
    "print(f\"Accuracy: {acc_t:.4f}\")\n",
    "print(f\"Precision (macro): {prec_t:.4f}\")\n",
    "print(f\"Recall (macro): {rec_t:.4f}\")\n",
    "print(f\"F1 (macro): {f1_t:.4f}\\n\")\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred_tuned, zero_division=0))\n",
    "\n",
    "cm_t = confusion_matrix(y_test, y_pred_tuned, labels=sorted(y.unique()))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73b5b6fd-c3ac-4375-90d3-b8a1ed855cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis - Engagement metrics such as Likes, Comments, and Streams do not impact the popularity of a track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a16ef904-cc6f-4c2a-9eac-383855ea6095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best params (no engagement): {'C': 10, 'class_weight': None, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best CV f1_macro (no engagement): 0.4775303364395006\n",
      "\n",
      "*****TUNED (NO ENGAGEMENT FEATURES) *****\n",
      "Accuracy: 0.5044\n",
      "Precision (macro): 0.4981\n",
      "Recall (macro): 0.5044\n",
      "F1 (macro): 0.4803\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        High       0.47      0.73      0.57      1350\n",
      "         Low       0.61      0.57      0.59      1350\n",
      "      Medium       0.41      0.21      0.28      1350\n",
      "\n",
      "    accuracy                           0.50      4050\n",
      "   macro avg       0.50      0.50      0.48      4050\n",
      "weighted avg       0.50      0.50      0.48      4050\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[990 140 220]\n",
      " [396 772 182]\n",
      " [722 347 281]]\n",
      "CHANGE IN METRICS AFTER FEATURE REMOVAL (WITH engagement) ===\n",
      "Accuracy Change: -0.3528\n",
      "Precision Change: -0.3618\n",
      "Recall Change: -0.3528\n",
      "F1 Change: -0.3769\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "target_col = 'popularity_class'\n",
    "\n",
    "#Dropping engagement features as per hypothesis\n",
    "df_no_engagement = df.drop(columns=['Likes', 'Comments', 'Stream'], errors='ignore').copy()\n",
    "\n",
    "#Train and Test split\n",
    "X2 = df_no_engagement.drop(columns=[target_col])\n",
    "y2 = df_no_engagement[target_col]\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.20, stratify=y2, random_state=RANDOM_STATE)\n",
    "\n",
    "#Identify Numeric column types\n",
    "num_cols2 = X2_train.select_dtypes(include=['float64', 'int64','bool']).columns.tolist()\n",
    "cat_cols2 = X2_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "#Scaling numeric features\n",
    "scaler2 = StandardScaler()\n",
    "X2_train_num = scaler2.fit_transform(X2_train[num_cols2])\n",
    "X2_test_num  = scaler2.transform(X2_test[num_cols2])\n",
    "X2_train_cat = X2_train[categorical_features]\n",
    "X2_test_cat = X2_test[categorical_features]\n",
    "\n",
    "#Recombining numerical + categorical data\n",
    "X2_train_final = hstack([X2_train_num, X2_train_cat])\n",
    "X2_test_final  = hstack([X2_test_num,  X2_test_cat])\n",
    "\n",
    "#Using GridSearchCV again to tune parameters as removing features may have changed the best params\n",
    "lr2 = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "\n",
    "param_grid2 = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'newton-cg', 'saga'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "\n",
    "grid2 = GridSearchCV(\n",
    "    estimator=lr2,\n",
    "    param_grid=param_grid2,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid2.fit(X2_train_final, y2_train)\n",
    "\n",
    "print(\"Best params (no engagement):\", grid2.best_params_)\n",
    "print(\"Best CV f1_macro (no engagement):\", grid2.best_score_)\n",
    "\n",
    "best_lr2 = grid2.best_estimator_\n",
    "y2_pred = best_lr2.predict(X2_test_final)\n",
    "\n",
    "acc2 = accuracy_score(y2_test, y2_pred)\n",
    "prec2, rec2, f12, _ = precision_recall_fscore_support(y2_test, y2_pred, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n*****TUNED (NO ENGAGEMENT FEATURES) *****\")\n",
    "print(f\"Accuracy: {acc2:.4f}\")\n",
    "print(f\"Precision (macro): {prec2:.4f}\")\n",
    "print(f\"Recall (macro): {rec2:.4f}\")\n",
    "print(f\"F1 (macro): {f12:.4f}\\n\")\n",
    "print(\"Classification report:\\n\", classification_report(y2_test, y2_pred, zero_division=0))\n",
    "\n",
    "cm2 = confusion_matrix(y2_test, y2_pred, labels=sorted(y2.unique()))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm2)\n",
    "\n",
    "#Comparison between previous model and post-hypothesis model\n",
    "print(\"CHANGE IN METRICS AFTER FEATURE REMOVAL (WITH engagement) ===\")\n",
    "print(f\"Accuracy Change: {acc2 - acc_t:+.4f}\")\n",
    "print(f\"Precision Change: {prec2 - prec_t:+.4f}\")\n",
    "print(f\"Recall Change: {rec2 - rec_t:+.4f}\")\n",
    "print(f\"F1 Change: {f12 - f1_t:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d81f9b2-c7ca-43bb-afeb-bf70706c5442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observations: \n",
    "#After feature removal, there is a huge drop in accuracy (from 85% to 50%), \n",
    "#F1 score also drops (from 0.85 to 0.48), confusion matrix shows many misclassifications for the Medium class.\n",
    "#Hence, engagement metrics such as Likes, Comments and Streams are highly predictive of popularity classes.\n",
    "#Without engagement metrics, Logistic Regression fails in distinguish classes, performing only slightly bettee than random guess, i.e, 33%\n",
    "#Hence, hypothesis is refused. Engagement metrics seems to be very important for model's ability to classify popularity of a track accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be9d750-6903-4b05-b632-1b59c00797e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
